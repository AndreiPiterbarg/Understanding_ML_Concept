{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreiPiterbarg/Understanding_ML_Concept/blob/main/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NtBU7ePNoKns"
      },
      "outputs": [],
      "source": [
        "%pip install mistralai --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqN8LAJ8tGq4"
      },
      "source": [
        "# Mastering Prompt Engineering\n",
        "In this notebook, you will practice systematically improving prompts to get better results from a Large Language Model.\n",
        "\n",
        "The goal of this assignment is to move from a vague, \"bad\" prompt to a precise, well-structured prompt that elicits a much more accurate and useful response from the model. You will also experiment with techniques that encourage the model to \"reason\" its way to a better answer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK2Y0VrPgGkG"
      },
      "source": [
        "## Part 1: Setting up a Mistral agent [5 points]\n",
        "To use the Mistral API, you need an API key.\n",
        "\n",
        "1. Create or sign in to your account at [console.mistral.ai](https://console.mistral.ai/).\n",
        "2. Open the **API Keys** tab and click **Create API key**. Give it a memorable name and copy the value.\n",
        "3. In this Colab notebook, click on the \"üîë\" (key) icon in the left sidebar.\n",
        "4. Click \"Add new secret\". Name the secret `MISTRAL_API_KEY` and paste your key into the \"Value\" field.\n",
        "5. Make sure the \"Notebook access\" toggle is turned on.\n",
        "\n",
        "By using Colab secrets, you keep your API key secure and avoid pasting it directly into your code. The code cell below will access this secret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Pci-vhgJf_ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e31634af-f05a-4c40-ccae-ec6666d8f11f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key configured successfully!\n"
          ]
        }
      ],
      "source": [
        "from mistralai import Mistral\n",
        "from google.colab import userdata\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Configure the API key\n",
        "try:\n",
        "    api_key = userdata.get('MISTRAL_API_KEY')\n",
        "    client = Mistral(api_key=api_key)\n",
        "    print(\"API Key configured successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring API key: {e}\")\n",
        "    print(\"Please make sure you have set the 'MISTRAL_API_KEY' secret in your Colab environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t28G6JY5sK_u"
      },
      "source": [
        "## 1.1 Agent Setup [5 points]\n",
        "Create a basic agent that sends a prompt to the Mistral model and returns a cleaned response. You should do the following:\n",
        "1. Create a generation config to set the temperature.\n",
        "2. Process the response to remove markdown formatting - any \"*\" or \"#\" characters\n",
        "3. Return the cleaned response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "QMXQmQ_ag4cL"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"mistral-small-latest\"\n",
        "\n",
        "def basic_agent(prompt: str, temperature=0.0) -> str:\n",
        "    \"\"\"Send a prompt to Mistral and return a markdown-free string response.\"\"\"\n",
        "    generation_config = {\"temperature\": float(temperature)}\n",
        "\n",
        "    response = client.chat.complete(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        **generation_config\n",
        "    )\n",
        "\n",
        "\n",
        "    text = response.choices[0].message.content if response and response.choices else \"\"\n",
        "    cleaned = re.sub(r\"[*#]\", \"\", text)\n",
        "    return cleaned.strip()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_json_output(output):\n",
        "    \"\"\"Parse JSON from model output, handling markdown wrappers.\"\"\"\n",
        "    try:\n",
        "        return json.loads(output)\n",
        "    except:\n",
        "        match = re.search(r'```(?:json)?\\s*([\\s\\S]*?)\\s*```', output)\n",
        "        if match:\n",
        "            try:\n",
        "                return json.loads(match.group(1))\n",
        "            except:\n",
        "                pass\n",
        "    return None"
      ],
      "metadata": {
        "id": "NhpMmIOivGXz"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYD_qbJhsjDp"
      },
      "source": [
        "## Part 2: Iterating and Improving a \"Bad\" Prompt [15 points]\n",
        "\n",
        "An LLM's output is only as good as the prompt it receives. Vague prompts lead to vague or incorrect answers. In this module, you will start with a deliberately \"bad\" prompt and improve it.\n",
        "\n",
        "**Task: Extract structured information from a block of text**\n",
        "\n",
        "Our goal is to process a chaotic meeting transcript from a supervillain's weekly sync. Evil plans have logistics too, and they're often messy. Your task is to extract key details and action items into a nested JSON object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "EzcSXkVMy65G"
      },
      "outputs": [],
      "source": [
        "supervillain_transcript = \"\"\"\n",
        "Project Doom-Spire - Weekly Status Update - Monday, February 9, 2025\n",
        "\n",
        "Attendees: Dr. Dire (Evil Overlord), Brenda (Lair HR & Logistics), Gary (Lead Henchman), Chad (Intern)\n",
        "\n",
        "Dr. Dire: People! The Doom-Spire is 80% complete, but I stood on the Parapet of Pain this morning and... it felt bland! It needs more... menace! Chad, you're the intern. Your youthful apathy is in tune with modern aesthetics. I want you to add more skulls to the parapet. Many more. Make it happen.\n",
        "\n",
        "Brenda: Um, Doctor? Speaking of making things happen, Unreliable Aquatics Inc. just called. The laser-equipped sharks for the moat are delayed. Again. They said \"next month, maybe.\" This is blocking the entire moat-filling initiative.\n",
        "\n",
        "Dr. Dire: Unacceptable! Brenda, find me a new shark supplier, ASAP. I don't care what it takes. Double our budget if you have to. I want sharks with lasers, and I want them yesterday!\n",
        "\n",
        "Gary: While we're on blockers, the primary laser core is overheating. It keeps melting the containment unit, which is, you know, suboptimal. We need a new cryo-cooler installed or the whole spire might just... pop.\n",
        "\n",
        "Dr. Dire: Pop? Gary, \"pop\" is not a word I want associated with my multi-billion dollar evil lair. Fine. Get the cooler. When can it be done?\n",
        "\n",
        "Gary: My team can have the new unit fully installed by this Friday.\n",
        "\n",
        "Dr. Dire: Good. See to it that it's done. I want no more talk of 'popping'. Okay team, good sync. Let's get back to menacing the world!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 1: The \"Bad\" Prompt\n",
        "bad_prompt = f\"\"\"\n",
        "What happened in this meeting?\n",
        "\n",
        "Transcript:\n",
        "{supervillain_transcript}\n",
        "\"\"\"\n",
        "\n",
        "output = basic_agent(bad_prompt)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJi50cJWjzfd",
        "outputId": "5d37eea6-c457-402d-ce93-7f8c177f883e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here‚Äôs a summary of what happened in the meeting:\n",
            "\n",
            "1. Project Status: The Doom-Spire is 80% complete, but Dr. Dire (the Evil Overlord) feels it lacks menace. He tasks Chad (the intern) with adding more skulls to the Parapet of Pain to enhance its ominous aesthetic.\n",
            "\n",
            "2. Moat Delays: Brenda (HR & Logistics) reports that the laser-equipped sharks for the moat are delayed indefinitely by the current supplier. Dr. Dire orders her to find a new supplier immediately, even if it means doubling the budget.\n",
            "\n",
            "3. Laser Core Issue: Gary (Lead Henchman) reveals that the primary laser core is overheating and melting its containment unit, risking a catastrophic \"pop.\" Dr. Dire insists on a new cryo-cooler installation by Friday to prevent this.\n",
            "\n",
            "4. Next Steps: The team is directed to focus on resolving these issues to ensure the Doom-Spire remains functional and intimidating.\n",
            "\n",
            "The meeting ends with Dr. Dire emphasizing the need to keep the project on track and maintain its menacing reputation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv2wXs8yzBRE"
      },
      "source": [
        "### 2.1 Adding Specificity and Structure [5 points]\n",
        "Version 1 summarizes the meeting but is impossible to parse. Rewrite the prompt in the next cell so the model returns the information in a nested JSON structure. We need the project name, a list of attendees with their titles, and a list of action items, where each action item is its own object.\n",
        "\n",
        "**Prompting Technique: Specificity and Schema Definition. Clearly define the entire output schema, including nested objects and arrays.**\n",
        "\n",
        "Your Task: Modify the prompt below to ask for a JSON object with the keys `project_name`, `attendees` (a list of objects with name and title), and `action_items` (a list of objects, where each object has `task_description`, `assigned_to`, and `due_date`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0bc7598"
      },
      "source": [
        "Hint: For reliable JSON, see Mistral‚Äôs Structured Outputs docs ([overview](https://docs.mistral.ai/capabilities/structured_output), [custom schemas](https://docs.mistral.ai/capabilities/structured_output/custom)) and JSON mode ([json mode](https://docs.mistral.ai/capabilities/structured_output/json_mode))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "0o9BZR9ty7lr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e7ebba6-890e-4e51-e34f-89d3a2bb0160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Final JSON Output:\n",
            "\n",
            "{\n",
            "  \"project_name\": \"Project Doom-Spire\",\n",
            "  \"attendees\": [\n",
            "    {\n",
            "      \"name\": \"Dr. Dire\",\n",
            "      \"title\": \"Evil Overlord\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Brenda\",\n",
            "      \"title\": \"Lair HR & Logistics\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Gary\",\n",
            "      \"title\": \"Lead Henchman\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Chad\",\n",
            "      \"title\": \"Intern\"\n",
            "    }\n",
            "  ],\n",
            "  \"action_items\": [\n",
            "    {\n",
            "      \"task_description\": \"Add more skulls to the parapet\",\n",
            "      \"assigned_to\": \"Chad\",\n",
            "      \"due_date\": null\n",
            "    },\n",
            "    {\n",
            "      \"task_description\": \"Find a new shark supplier\",\n",
            "      \"assigned_to\": \"Brenda\",\n",
            "      \"due_date\": null\n",
            "    },\n",
            "    {\n",
            "      \"task_description\": \"Install the new cryo-cooler\",\n",
            "      \"assigned_to\": \"Gary\",\n",
            "      \"due_date\": \"this Friday\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Version 2: Specifying the Nested Schema\n",
        "schema_prompt = f\"\"\"\n",
        "You are an information extraction assistant and you are to follow the specified schema EXACTLY:\n",
        "\n",
        "You will return a JSON object with the keys project_name, attendees (a list of objects with name and title), and action_items (a list of objects, where each object has task_description, assigned_to, and due_date).\n",
        "\n",
        "Rules:\n",
        "- Only use information present in the transcript.\n",
        "- Put every action item as its own object.\n",
        "- If an action item does not have an explicit due date in the transcript, set \"due_date\" to null.\n",
        "- Output JSON ONLY.\n",
        "\n",
        "Transcript:\n",
        "{supervillain_transcript}\n",
        "\"\"\"\n",
        "\n",
        "output = basic_agent(schema_prompt)\n",
        "\n",
        "# Clean the output for JSON parsing\n",
        "try:\n",
        "    cleaned_output = re.search(r'```json\\n(.*)\\n```', output, re.DOTALL).group(1)\n",
        "    parsed_json = json.loads(cleaned_output)\n",
        "    print(\"\\n‚úÖ Final JSON Output:\\n\")\n",
        "    print(json.dumps(parsed_json, indent=2))\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Output is NOT valid nested JSON. Raw output:\\n{output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJBmINdNzNSX"
      },
      "source": [
        "### 2.2 Handling Ambiguity and Inference [5 points]\n",
        "The schema is correct, but the model still struggles with ambiguity. Update the prompt in the next cell so it states the following rules explicitly:\n",
        "1. Treat Monday, February 9, 2025 as the reference date for every due date calculation.\n",
        "2. Map \"ASAP\" to the next business day.\n",
        "3. Use `null` when no deadline appears in the transcript.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "9PyRGBPJzSCu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1b35db9-56dc-481b-fc15-a0bbce32fe70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Final JSON Output:\n",
            "\n",
            "{\n",
            "  \"project_name\": \"Doom-Spire\",\n",
            "  \"attendees\": [\n",
            "    {\n",
            "      \"name\": \"Dr. Dire\",\n",
            "      \"title\": \"Evil Overlord\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Brenda\",\n",
            "      \"title\": \"Lair HR & Logistics\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Gary\",\n",
            "      \"title\": \"Lead Henchman\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Chad\",\n",
            "      \"title\": \"Intern\"\n",
            "    }\n",
            "  ],\n",
            "  \"action_items\": [\n",
            "    {\n",
            "      \"task_description\": \"Add more skulls to the parapet\",\n",
            "      \"assigned_to\": \"Chad\",\n",
            "      \"due_date\": null\n",
            "    },\n",
            "    {\n",
            "      \"task_description\": \"Find a new shark supplier\",\n",
            "      \"assigned_to\": \"Brenda\",\n",
            "      \"due_date\": \"2025-02-10\"\n",
            "    },\n",
            "    {\n",
            "      \"task_description\": \"Install new cryo-cooler\",\n",
            "      \"assigned_to\": \"Gary\",\n",
            "      \"due_date\": \"2025-02-14\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Version 3: Handling Ambiguity\n",
        "ambiguity_prompt = f\"\"\"\n",
        "\n",
        "Goal: Extract structured information from the transcript into **valid JSON only**, using EXACTLY this schema:\n",
        "\n",
        "{{\n",
        "  \"project_name\": string,\n",
        "  \"attendees\": [\n",
        "    {{\n",
        "      \"name\": string,\n",
        "      \"title\": string\n",
        "    }}\n",
        "  ],\n",
        "  \"action_items\": [\n",
        "    {{\n",
        "      \"task_description\": string,\n",
        "      \"assigned_to\": string,\n",
        "      \"due_date\": string | null\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Hard rules (must follow):\n",
        "1) Treat \"Monday, February 9, 2025\" as the meeting/reference date for all due-date calculations.\n",
        "2) Map \"ASAP\" to the next business day: 2025-02-10.\n",
        "3) Map \"by this Friday\" / \"by Friday\" to 2025-02-14.\n",
        "4) If no deadline appears for an action item, set \"due_date\" to null.\n",
        "5) Dates must be in YYYY-MM-DD format.\n",
        "6) Only use information present in the transcript (no guessing beyond the rules above).\n",
        "7) Output JSON ONLY .\n",
        "\n",
        "\n",
        "Transcript:\n",
        "{supervillain_transcript}\n",
        "\"\"\"\n",
        "\n",
        "output = basic_agent(ambiguity_prompt)\n",
        "\n",
        "# Clean the output for JSON parsing\n",
        "try:\n",
        "    # The model sometimes wraps the JSON in markdown ` ```json ... ``` `\n",
        "    cleaned_output = re.search(r'```json\\n(.*)\\n```', output, re.DOTALL).group(1)\n",
        "    parsed_json = json.loads(cleaned_output)\n",
        "    print(\"\\n‚úÖ Final JSON Output:\\n\")\n",
        "    print(json.dumps(parsed_json, indent=2))\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Output is NOT valid nested JSON. Raw output:\\n{output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-chT70r5QdB"
      },
      "source": [
        "### 2.3 Automatic Prompt Engineering (Meta-Prompting) [5 points]\n",
        "The final step is to automate the improvements you just made. You will:\n",
        "- write a meta-prompt that teaches `prompt_agent` how to craft the final schema-aware prompt\n",
        "- require the meta-prompt to mention the persona, schema, reference date, ambiguity rules, and JSON-only constraint\n",
        "- append the transcript to the generated prompt before calling `basic_agent`\n",
        "\n",
        "Effectively, you are building a helper agent. Make the instructions so unambiguous that a second model can follow them without you editing the prompt by hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "bWILjssxRU5b"
      },
      "outputs": [],
      "source": [
        "def prompt_agent(meta_prompt: str, transcript: str) -> str:\n",
        "    \"\"\"Generate a refined prompt and append the transcript.\"\"\"\n",
        "    improved_instructions = basic_agent(meta_prompt)\n",
        "    return f\"{improved_instructions}\\n\\nTranscript:\\n{transcript}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDPXKZ2GR9Xk"
      },
      "source": [
        "**Your Task:** Fill in `meta_prompt` so that `prompt_agent(meta_prompt, supervillain_transcript)` produces a final prompt containing all of the following:\n",
        "1. A persona assignment (e.g., expert project manager bot).\n",
        "2. A clear restatement of the goal: extract structured JSON.\n",
        "3. The exact schema: `project_name`, `attendees` with `name` + `title`, and `action_items` with `task_description`, `assigned_to`, `due_date`.\n",
        "4. The reference date (Monday, February 9, 2025).\n",
        "5. The ambiguity rules for \"ASAP\" and \"by Friday\" and `null` for missing dates.\n",
        "6. An explicit instruction to output JSON only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "zcpuzi1BSTeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54ad95db-8018-48b2-a78b-26427adadf24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generating a new prompt using the prompt_agent... ---\n",
            "```json\n",
            "{\n",
            "  \"role\": \"expert project manager bot\",\n",
            "  \"goal\": \"extract structured JSON from meeting transcript\",\n",
            "  \"schema\": {\n",
            "    \"project_name\": \"string\",\n",
            "    \"attendees\": [\n",
            "      {\n",
            "        \"name\": \"string\",\n",
            "        \"title\": \"string\"\n",
            "      }\n",
            "    ],\n",
            "    \"action_items\": [\n",
            "      {\n",
            "        \"task_description\": \"string\",\n",
            "        \"assigned_to\": \"string\",\n",
            "        \"due_date\": \"YYYY-MM-DD\"\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"reference_date\": \"2025-02-09\",\n",
            "  \"ambiguity_rules\": {\n",
            "    \"ASAP\": \"2025-02-10\",\n",
            "    \"by Friday\": \"2025-02-14\",\n",
            "    \"missing\": null,\n",
            "    \"format\": \"YYYY-MM-DD\"\n",
            "  },\n",
            "  \"output_format\": \"JSON only (no markdown)\",\n",
            "  \"transcript\": \"APPEND TRANSCRIPT HERE\"\n",
            "}\n",
            "```\n",
            "\n",
            "Transcript:\n",
            "\n",
            "Project Doom-Spire - Weekly Status Update - Monday, February 9, 2025\n",
            "\n",
            "Attendees: Dr. Dire (Evil Overlord), Brenda (Lair HR & Logistics), Gary (Lead Henchman), Chad (Intern)\n",
            "\n",
            "Dr. Dire: People! The Doom-Spire is 80% complete, but I stood on the Parapet of Pain this morning and... it felt bland! It needs more... menace! Chad, you're the intern. Your youthful apathy is in tune with modern aesthetics. I want you to add more skulls to the parapet. Many more. Make it happen.\n",
            "\n",
            "Brenda: Um, Doctor? Speaking of making things happen, Unreliable Aquatics Inc. just called. The laser-equipped sharks for the moat are delayed. Again. They said \"next month, maybe.\" This is blocking the entire moat-filling initiative.\n",
            "\n",
            "Dr. Dire: Unacceptable! Brenda, find me a new shark supplier, ASAP. I don't care what it takes. Double our budget if you have to. I want sharks with lasers, and I want them yesterday!\n",
            "\n",
            "Gary: While we're on blockers, the primary laser core is overheating. It keeps melting the containment unit, which is, you know, suboptimal. We need a new cryo-cooler installed or the whole spire might just... pop.\n",
            "\n",
            "Dr. Dire: Pop? Gary, \"pop\" is not a word I want associated with my multi-billion dollar evil lair. Fine. Get the cooler. When can it be done?\n",
            "\n",
            "Gary: My team can have the new unit fully installed by this Friday.\n",
            "\n",
            "Dr. Dire: Good. See to it that it's done. I want no more talk of 'popping'. Okay team, good sync. Let's get back to menacing the world!\n",
            "\n",
            "----------------------------------------------------\n",
            "\n",
            "\n",
            "--- Using the generated prompt to extract data... ---\n",
            "\n",
            "‚úÖ Final JSON Output:\n",
            "\n",
            "{\n",
            "  \"project_name\": \"Project Doom-Spire\",\n",
            "  \"attendees\": [\n",
            "    {\n",
            "      \"name\": \"Dr. Dire\",\n",
            "      \"title\": \"Evil Overlord\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Brenda\",\n",
            "      \"title\": \"Lair HR & Logistics\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Gary\",\n",
            "      \"title\": \"Lead Henchman\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Chad\",\n",
            "      \"title\": \"Intern\"\n",
            "    }\n",
            "  ],\n",
            "  \"action_items\": [\n",
            "    {\n",
            "      \"task_description\": \"Add more skulls to the parapet\",\n",
            "      \"assigned_to\": \"Chad\",\n",
            "      \"due_date\": null\n",
            "    },\n",
            "    {\n",
            "      \"task_description\": \"Find a new shark supplier for the moat\",\n",
            "      \"assigned_to\": \"Brenda\",\n",
            "      \"due_date\": \"2025-02-10\"\n",
            "    },\n",
            "    {\n",
            "      \"task_description\": \"Install new cryo-cooler for the primary laser core\",\n",
            "      \"assigned_to\": \"Gary\",\n",
            "      \"due_date\": \"2025-02-14\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "meta_prompt = \"\"\"Write a prompt for extracting meeting data as JSON. Include:\n",
        "1. Persona: expert project manager bot\n",
        "2. Goal: extract structured JSON\n",
        "3. Schema: project_name (string), attendees (list with name/title), action_items (list with task_description/assigned_to/due_date)\n",
        "4. Reference date: Monday, February 9, 2025\n",
        "5. Ambiguity rules: ASAP=2025-02-10, by Friday=2025-02-14, missing=null, format=YYYY-MM-DD\n",
        "6. Output JSON only (no markdown)\n",
        "\n",
        "Output ONLY the prompt text. The transcript will be appended separately.\"\"\"\n",
        "\n",
        "# --- Let's run the two-step process ---\n",
        "\n",
        "# Step 1: Use the meta-prompt to have the prompt_agent generate a new prompt for us.\n",
        "print(\"--- Generating a new prompt using the prompt_agent... ---\")\n",
        "generated_prompt = prompt_agent(meta_prompt, supervillain_transcript)\n",
        "print(generated_prompt)\n",
        "print(\"----------------------------------------------------\")\n",
        "\n",
        "\n",
        "# Step 2: Use the newly generated prompt to process the transcript with our original agent.\n",
        "print(\"\\n\\n--- Using the generated prompt to extract data... ---\")\n",
        "# We need to append the transcript to the prompt that was just generated\n",
        "final_output = basic_agent(generated_prompt)\n",
        "\n",
        "# Clean the output for JSON parsing\n",
        "try:\n",
        "    cleaned_output = re.search(r'```json\\n(.*)\\n```', final_output, re.DOTALL).group(1)\n",
        "    parsed_json = json.loads(cleaned_output)\n",
        "    print(\"\\n‚úÖ Final JSON Output:\\n\")\n",
        "    print(json.dumps(parsed_json, indent=2))\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Output is NOT valid nested JSON. Raw output:\\n{final_output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPEykj9848Iz"
      },
      "source": [
        "# Part 3: Eliciting Reasoning [20 points]\n",
        "You will practice two reasoning-friendly prompt patterns: Chain-of-Thought for constraint solving and Reflection for self-critique. Each sub-part has a single correct answer, so your prompts must spell out how the model should reason before giving the final response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J4I_4sZHd6T"
      },
      "source": [
        "## 3.1: Chain-of-Thought (CoT) for Complex Problems [10 points]\n",
        "\n",
        "For complex problems, like logic puzzles, simply asking for the answer can be unreliable. The Chain-of-Thought (CoT) technique guides the model to \"think step-by-step,\" breaking down the problem and showing its work. This reasoning process dramatically increases the likelihood of arriving at the correct answer.\n",
        "\n",
        "The Task: Solve a moderately difficult logic puzzle with a single correct solution.\n",
        "\n",
        "*Four wizards are discussing their magical pets. The wizards are Arthur, Beatrice, Cassandra, and Desmond. The pets are an Owl, a Griffin, a Phoenix, and a Dragon. Each wizard owns exactly one pet*\n",
        "* *Constraint 1: Beatrice owns the Dragon.*\n",
        "* *Constraint 2: The owner of the Griffin is not Cassandra or Desmond.*\n",
        "* *Constraint 3: Arthur does not own the Phoenix.*\n",
        "* *Constraint 4: Cassandra's pet is not the Owl.*\n",
        "\n",
        "**Your Task: Write a prompt that uses the Chain-of-Thought technique to solve this puzzle. The prompt should instruct the model to first lay out the facts, then use a process of elimination, and finally state the answer in a JSON format.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "tP-GOqRD5HcM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d67f75f-96f9-4d73-9853-c4af2afe2ee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: List the given facts/constraints\n",
            "- Constraint 1: Beatrice owns the Dragon.\n",
            "- Constraint 2: The owner of the Griffin is not Cassandra or Desmond.\n",
            "- Constraint 3: Arthur does not own the Phoenix.\n",
            "- Constraint 4: Cassandra's pet is not the Owl.\n",
            "\n",
            " Step 2: Create an elimination table\n",
            "We'll represent the possible pets for each wizard based on the constraints.\n",
            "\n",
            "| Wizard      | Owl | Griffin | Phoenix | Dragon |\n",
            "|-------------|-----|---------|---------|--------|\n",
            "| Arthur      | ?   | ?       | X       | ?      |\n",
            "| Beatrice    | ?   | ?       | ?       | ‚úì      |\n",
            "| Cassandra   | X   | X       | ?       | ?      |\n",
            "| Desmond     | ?   | X       | ?       | ?      |\n",
            "\n",
            " Step 3: Process of elimination\n",
            "1. Constraint 1: Beatrice owns the Dragon.\n",
            "   - Beatrice's row: Owl (X), Griffin (X), Phoenix (X), Dragon (‚úì).\n",
            "   - Other wizards cannot own the Dragon.\n",
            "\n",
            "2. Constraint 2: The owner of the Griffin is not Cassandra or Desmond.\n",
            "   - Cassandra's row: Griffin (X).\n",
            "   - Desmond's row: Griffin (X).\n",
            "   - So, the Griffin must be owned by Arthur or Beatrice.\n",
            "   - But Beatrice already owns the Dragon, so the Griffin must be owned by Arthur.\n",
            "\n",
            "3. Constraint 3: Arthur does not own the Phoenix.\n",
            "   - Arthur's row: Phoenix (X).\n",
            "   - Arthur owns the Griffin (from step 2), so this is already satisfied.\n",
            "\n",
            "4. Constraint 4: Cassandra's pet is not the Owl.\n",
            "   - Cassandra's row: Owl (X).\n",
            "   - Cassandra cannot own the Owl or Griffin (from Constraint 2), so Cassandra must own the Phoenix.\n",
            "\n",
            "5. Now, assign the remaining pet to Desmond.\n",
            "   - The remaining pet is the Owl (since Beatrice has Dragon, Arthur has Griffin, Cassandra has Phoenix).\n",
            "   - Desmond's row: Owl (‚úì).\n",
            "\n",
            " Step 4: Double-check constraints\n",
            "- Constraint 1: Beatrice owns the Dragon. ‚úì\n",
            "- Constraint 2: The owner of the Griffin (Arthur) is not Cassandra or Desmond. ‚úì\n",
            "- Constraint 3: Arthur does not own the Phoenix. ‚úì\n",
            "- Constraint 4: Cassandra's pet (Phoenix) is not the Owl. ‚úì\n",
            "\n",
            " Final Answer:\n",
            "```json\n",
            "{\n",
            "  \"Arthur\": \"Griffin\",\n",
            "  \"Beatrice\": \"Dragon\",\n",
            "  \"Cassandra\": \"Phoenix\",\n",
            "  \"Desmond\": \"Owl\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "logic_puzzle = \"\"\"\n",
        "Four wizards are discussing their magical pets. The wizards are Arthur, Beatrice, Cassandra, and Desmond. The pets are an Owl, a Griffin, a Phoenix, and a Dragon. Each wizard owns exactly one pet.\n",
        "- Constraint 1: Beatrice owns the Dragon.\n",
        "- Constraint 2: The owner of the Griffin is not Cassandra or Desmond.\n",
        "- Constraint 3: Arthur does not own the Phoenix.\n",
        "- Constraint 4: Cassandra's pet is not the Owl.\n",
        "\"\"\"\n",
        "\n",
        "cot_puzzle_prompt = f\"\"\"\n",
        "You are a logic-solver.\n",
        "\n",
        "Solve the puzzle by SHOWING YOUR WORK at each step:\n",
        "1) List the given facts/constraints as bullet points.\n",
        "2) Create a small elimination table (who can/can't own each pet).\n",
        "3) Use process-of-elimination, one inference per step, until you reach the only consistent assignment.\n",
        "4) Double-check every constraint against your final assignment.\n",
        "\n",
        "Finally, output ONLY valid JSON in this exact format (no markdown, no extra text):\n",
        "{{\n",
        "  \"Arthur\": \"<pet>\",\n",
        "  \"Beatrice\": \"<pet>\",\n",
        "  \"Cassandra\": \"<pet>\",\n",
        "  \"Desmond\": \"<pet>\"\n",
        "}}\n",
        "\n",
        "Puzzle:\n",
        "{logic_puzzle}\n",
        "\"\"\"\n",
        "\n",
        "output = basic_agent(cot_puzzle_prompt)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFjYVuafgZC8"
      },
      "source": [
        "## 3.2 Reflection and Refinement (Self-Correction) [10 points]\n",
        "Write a single prompt that makes the model critique its own startup idea in three clear passes:\n",
        "1. *Step 1 ‚Äì Ideation:* act as an optimistic founder, name the app, and describe the core function in one sentence.\n",
        "2. *Step 2 ‚Äì Skeptical VC:* switch personas and answer the classic diligence questions (problem urgency, current alternatives, market size, competitors & advantage, monetization).\n",
        "3. *Step 3 ‚Äì Improved Concept:* return to the founder persona, fix every weakness the VC raised, and explain why the new version is more defensible.\n",
        "\n",
        "**Your Task: Create a three prompts, one for each step of the process. Call the basic_agent three times to perform each step of this process**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "MLhm8vTPBSam",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a304b00-2f32-455c-c2e0-f45de282dc6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "App Name: ThriveTogether\n",
            "\n",
            "Core Function: ThriveTogether connects remote workers with local co-working buddies in their city, matching them based on work styles and schedules to combat loneliness and boost productivity through in-person meetups.\n",
            "\n",
            "1) Problem urgency: Remote workers who feel isolated or unproductive due to lack of social interaction might find this useful, but it‚Äôs not a life-or-death problem. The urgency depends on the individual‚Äîsome thrive alone, others crave connection. Pain level is moderate, not urgent.\n",
            "\n",
            "2) Current alternatives: People already use Slack/Discord communities, local meetup groups, or coworking spaces. Many remote workers also just work from cafes or libraries. The main difference is ThriveTogether‚Äôs focus on curated 1:1 or small-group matchmaking.\n",
            "\n",
            "3) Market size: Medium to niche. Remote work is growing, but the subset of people who want in-person meetups with strangers is smaller. The total addressable market is large (millions of remote workers), but the active market is likely in the hundreds of thousands.\n",
            "\n",
            "4) Competitors & advantage: Competitors include coworking spaces (WeWork, local spots), networking apps (Bumble BFF, Meetup), and remote work communities (Nomad List, Remote Year). ThriveTogether‚Äôs edge is the focus on work-style matching, but this is hard to scale without deep data. If it‚Äôs just a dating-app-for-friends, it‚Äôs easily replicated.\n",
            "\n",
            "5) Monetization: Likely freemium with premium features (e.g., advanced matching, event hosting). Pricing risks: Users may not pay for something they can get for free elsewhere (e.g., Meetup). If adoption is low, the network effect fails, and the app becomes useless.\n",
            "\n",
            "App Name: ThriveTogether Pro\n",
            "\n",
            "Core Function: ThriveTogether Pro is a premium productivity and social platform that intelligently matches remote workers with local co-working partners based on work habits, goals, and personality‚Äîdelivering measurable productivity gains and meaningful connections.\n",
            "\n",
            "How we fixed the VC‚Äôs concerns:\n",
            "\n",
            "(a) Urgency: We now highlight that loneliness and productivity loss cost remote workers $X/year in lost earnings and well-being (citing studies). The app offers a structured, time-bound solution (e.g., \"30-day productivity boost\") to make the need feel urgent.\n",
            "\n",
            "(b) Alternatives: We differentiate by offering AI-driven work-style matching (not just casual meetups) and integrating productivity tools (e.g., shared goal trackers, accountability partners). Slack/Discord lack this focus, and coworking spaces are expensive.\n",
            "\n",
            "(c) Market size: We target the 50M+ remote workers in the U.S. alone, focusing on high-paying professionals (e.g., tech, consulting) who prioritize productivity and networking‚Äîexpanding the active market to millions.\n",
            "\n",
            "(d) Competitors/advantage: We‚Äôve built proprietary matching algorithms using work habit data (e.g., deep work hours, collaboration styles) and partnered with coworking spaces for exclusive perks. Unlike Bumble BFF, we‚Äôre a productivity-first tool with verifiable ROI.\n",
            "\n",
            "(e) Monetization: Premium tiers unlock advanced analytics (e.g., productivity reports), priority matching, and access to elite coworking spaces. We also monetize via partnerships (e.g., ergonomic gear discounts) and enterprise plans for remote teams.\n"
          ]
        }
      ],
      "source": [
        "step1_prompt = \"\"\"\n",
        "  You are an optimistic startup founder.\n",
        "\n",
        "Task:\n",
        "- Invent ONE original new startup app idea.\n",
        "- Name the app.\n",
        "- Describe the core function in EXACTLY one sentence.\n",
        "\n",
        "Constraints:\n",
        "- Be specific about the user and the problem.\n",
        "- Output ONLY plain text (no markdown, no lists).\n",
        "\"\"\"\n",
        "\n",
        "step1_output = basic_agent(step1_prompt)\n",
        "print(step1_output)\n",
        "\n",
        "step2_prompt = f\"\"\"\n",
        "  You are a skeptical VC doing due diligence.\n",
        "\n",
        "Critique the concept below by answering these questions clearly:\n",
        "1) Problem urgency: who has this problem and how painful/urgent is it?\n",
        "2) Current alternatives: what do people use today instead?\n",
        "3) Market size: is this niche, medium, or huge? Why?\n",
        "4) Competitors & advantage: who would compete and what is this concept's edge (or lack of one)?\n",
        "5) Monetization: how would it make money, and what are pricing risks?\n",
        "\n",
        "Be honest and specific. Output plain text only (no markdown).\n",
        "\n",
        "Concept to critique:\n",
        "{step1_output}\n",
        "\"\"\"\n",
        "\n",
        "step2_output = basic_agent(step2_prompt)\n",
        "print(\"\\n\" + step2_output)\n",
        "\n",
        "step3_prompt = f\"\"\"\n",
        "You are an optimistic founder, and you must FIX every weakness raised by the VC.\n",
        "\n",
        "Instructions:\n",
        "- Start by giving the improved app a name (can be the same or new).\n",
        "- Provide an improved one-sentence core function.\n",
        "- Then, in 5 short bullets (one per item), explain how the new version addresses:\n",
        "  (a) urgency, (b) alternatives, (c) market size, (d) competitors/advantage, (e) monetization.\n",
        "- You MUST explicitly reference points from the VC critique and show how you resolved them.\n",
        "\n",
        "Output plain text only (no markdown).\n",
        "\n",
        "Original concept:\n",
        "{step1_output}\n",
        "\n",
        "VC critique:\n",
        "{step2_output}\n",
        "\"\"\"\n",
        "\n",
        "step3_output = basic_agent(step3_prompt)\n",
        "print(\"\\n\" + step3_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR3F5tiVBSam"
      },
      "source": [
        "# Part 4: Conversational Memory [30 points]\n",
        "You will build a three-layer travel concierge stack:\n",
        "1. Buffer the entire conversation so every reply remembers earlier preferences.\n",
        "2. Convert those turns into a structured memory store of durable trip facts.\n",
        "3. Feed the memory summaries back into the next response so the concierge stays consistent.\n",
        "\n",
        "Complete the steps in order‚Äîeach helper you write is reused in the following section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5pjLGS1BSam"
      },
      "source": [
        "## 4.1 Conversation Buffer [10 points]\n",
        "Implement two helpers that keep the running dialogue intact:\n",
        "- `format_transcript_for_prompt(messages)` ‚Üí return a newline-delimited string of labeled turns plus an instruction telling the assistant to continue.\n",
        "- `run_buffered_conversation(system_context, user_turns)` ‚Üí step through each user turn, rebuild the prompt with the full transcript, call `basic_agent`, store every reply, and return both the transcript and a dialogue log.\n",
        "\n",
        "Then, run the provided travel scenario and print both a single-shot response and the buffered dialogue so you can highlight why the buffered agent performs better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "sWdf7SviBSan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "825a4134-c184-4fae-d2d7-b55d8157d817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single-shot response (no buffer):\n",
            "\n",
            "3-Day Florence Itinerary\n",
            "\n",
            "Day 1-2: Florence\n",
            "- Stay at Hotel Brunelleschi (boutique, near Duomo & Uffizi).\n",
            "- Must-sees: Uffizi Gallery, Accademia (David), Ponte Vecchio.\n",
            "- Dinner at Trattoria Mario (local, no reservations).\n",
            "\n",
            "Day 3: Tuscany Day Trip\n",
            "- Drive to San Gimignano (1.5 hrs). Cooking class at Tuscany Cooking Class (farm-to-table pasta).\n",
            "- Return to Florence by evening.\n",
            "\n",
            "Notes:\n",
            "- Pack light for transit; no checked bags needed.\n",
            "- Notify hotel of dietary needs (e.g., gluten-free) 48hrs ahead.\n",
            "\n",
            "---\n",
            "\n",
            "Buffered dialogue:\n",
            "Turn 1 | User: Hi, we're planning a June 10‚Äì16 trip to Italy. We'd like three nights in Rome and three in Florence.\n",
            "Turn 1 | Assistant: Got it! For your June 10‚Äì16 Italy trip:\n",
            "- Rome (June 10‚Äì13): Must-see spots? Colosseum, Vatican, Trevi Fountain.\n",
            "- Florence (June 13‚Äì16): Duomo, Uffizi Gallery, Ponte Vecchio.\n",
            "- Any dietary restrictions or transit preferences (e.g., trains vs. flights)? I‚Äôll refine your itinerary!\n",
            "\n",
            "Turn 2 | User: Please avoid redeye flights, schedule trains before 8pm, and remember I'm allergic to shellfish.\n",
            "Turn 2 | Assistant: Understood! Your Italy trip (June 10‚Äì16) will avoid redeye flights, schedule trains before 8pm, and exclude shellfish. I‚Äôll plan:\n",
            "- Rome (June 10‚Äì13): Colosseum, Vatican, Trevi Fountain (shellfish-free dining).\n",
            "- Florence (June 13‚Äì16): Duomo, Uffizi, Ponte Vecchio (train from Rome by 7pm).\n",
            "Need hotel/activity preferences?\n",
            "\n",
            "Turn 3 | User: We love boutique hotels near museums and want one day trip to Tuscany with a local cooking class. Can you sketch the plan?\n",
            "Turn 3 | Assistant: Here‚Äôs your refined Italy plan (June 10‚Äì16):\n",
            "- Rome (June 10‚Äì13): Boutique hotel near Vatican. Day 1: Colosseum + Trevi Fountain. Day 2: Vatican + shellfish-free trattoria. Day 3: Train to Florence (6pm).\n",
            "- Florence (June 13‚Äì16): Boutique hotel near Uffizi. Day 4: Duomo + cooking class in Tuscany (e.g., Chianti). Day 5: Uffizi + Ponte Vecchio. Day 6: Depart.\n",
            "Need specific hotel names or cooking class details?\n",
            "\n",
            "Final buffered reply clearly references the earlier turns.\n"
          ]
        }
      ],
      "source": [
        "def format_transcript_for_prompt(messages):\n",
        "    \"\"\"Return a speaker-labeled transcript string that becomes the next prompt.\"\"\"\n",
        "    lines = []\n",
        "    for m in messages:\n",
        "        role = m[\"role\"].capitalize()\n",
        "        lines.append(f\"{role}: {m['content']}\")\n",
        "    lines.append(\"Assistant: Continue the conversation using the full transcript above. Keep your reply concise.\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def run_buffered_conversation(system_context, user_turns):\n",
        "    \"\"\"Return the updated transcript and dialogue history after iterating over every user turn.\"\"\"\n",
        "    messages = [{\"role\": \"system\", \"content\": system_context}]\n",
        "    dialogue_log = []\n",
        "\n",
        "    for user_msg in user_turns:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        prompt = format_transcript_for_prompt(messages)\n",
        "        assistant_msg = basic_agent(prompt)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "        dialogue_log.append({\"user\": user_msg, \"assistant\": assistant_msg})\n",
        "\n",
        "    transcript = format_transcript_for_prompt(messages)\n",
        "    return transcript, dialogue_log\n",
        "\n",
        "\n",
        "travel_system_prompt = (\n",
        "  \"You are a detail-oriented travel concierge. Track traveler dates, transit rules, dietary needs, and must-see spots so itineraries stay consistent. Keep replies under 120 words.\"\n",
        ")\n",
        "travel_turns = [\n",
        "  \"Hi, we're planning a June 10‚Äì16 trip to Italy. We'd like three nights in Rome and three in Florence.\",\n",
        "  \"Please avoid redeye flights, schedule trains before 8pm, and remember I'm allergic to shellfish.\",\n",
        "  \"We love boutique hotels near museums and want one day trip to Tuscany with a local cooking class. Can you sketch the plan?\"\n",
        "]\n",
        "\n",
        "buffered_transcript, buffered_dialogue = run_buffered_conversation(\n",
        "  travel_system_prompt,\n",
        "  travel_turns\n",
        ")\n",
        "\n",
        "single_shot_prompt = (\n",
        "    f\"{travel_system_prompt}\\n\"\n",
        "    f\"User: {travel_turns[-1]}\\n\"\n",
        "    \"Assistant:\"\n",
        ")\n",
        "single_shot_response = basic_agent(single_shot_prompt)\n",
        "\n",
        "print(\"Single-shot response (no buffer):\\n\")\n",
        "print(single_shot_response)\n",
        "print(\"\\n---\\n\")\n",
        "print(\"Buffered dialogue:\")\n",
        "for idx, turn in enumerate(buffered_dialogue, start=1):\n",
        "    print(f\"Turn {idx} | User: {turn['user']}\")\n",
        "    print(f\"Turn {idx} | Assistant: {turn['assistant']}\\n\")\n",
        "print(\"Final buffered reply clearly references the earlier turns.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMnss5WDBSan"
      },
      "source": [
        "## 4.2 Structured Memory Store (Fact Capture) [10 points]\n",
        "Now that buffering works, capture persistent travel facts after every turn.\n",
        "- `extract_conversation_memories(user_message, assistant_message)` ‚Üí call the model to emit a JSON list of durable facts.\n",
        "- `update_memory_store(store, memories)` ‚Üí merge those facts by topic while deduplicating details.\n",
        "- `run_memory_capture_session(system_context, user_turns)` ‚Üí reuse the buffer helpers, call the extractor after each reply, print the evolving store, and return both the transcript and the final `memory_store`.\n",
        "\n",
        "Aim for a store that records dates, budgets, dietary needs, transit rules, and backup plans mentioned in the scenario.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "yKVGIpIiHd6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d73a16-29ef-466b-d341-cb09d4efd883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Turn 1...\n",
            "\n",
            "Updated Store:\n",
            "{\n",
            "  \"dates\": [\n",
            "    \"June 10-16\"\n",
            "  ],\n",
            "  \"destinations\": [\n",
            "    \"Italy\"\n",
            "  ],\n",
            "  \"budget\": [\n",
            "    \"tight\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_conversation_memories(user_message, assistant_message):\n",
        "    \"\"\"Use an LLM to extract key facts into a JSON list: [{'topic': ..., 'detail': ...}]\"\"\"\n",
        "    extractor_prompt = f\"\"\"Extract travel facts as JSON list: [{{\"topic\": \"...\", \"detail\": \"...\"}}]\n",
        "\n",
        "Topics to look for: dates, destinations, budget, dietary, transit_rules, lodging_preferences, activities, backup_plans\n",
        "Only include concrete facts. Output JSON only (no markdown).\n",
        "\n",
        "User: {user_message}\n",
        "Assistant: {assistant_message}\"\"\"\n",
        "\n",
        "    output = basic_agent(extractor_prompt)\n",
        "    parsed = parse_json_output(output)\n",
        "\n",
        "    if isinstance(parsed, list):\n",
        "        return [m for m in parsed if isinstance(m, dict) and \"topic\" in m and \"detail\" in m]\n",
        "    return []\n",
        "\n",
        "\n",
        "def update_memory_store(store, memories):\n",
        "    \"\"\"Merge new memories into the store, avoiding duplicates.\"\"\"\n",
        "    if store is None:\n",
        "        store = {}\n",
        "    for mem in memories or []:\n",
        "        topic = mem.get(\"topic\", \"\").lower().strip()\n",
        "        detail = mem.get(\"detail\", \"\").strip()\n",
        "        if not topic or not detail:\n",
        "            continue\n",
        "        if topic not in store:\n",
        "            store[topic] = []\n",
        "        if detail.lower() not in [d.lower() for d in store[topic]]:\n",
        "            store[topic].append(detail)\n",
        "    return store\n",
        "\n",
        "# --- Test Logic ---\n",
        "travel_transcript = []\n",
        "travel_memory_store = {}\n",
        "\n",
        "turn_1 = \"I'm planning a trip to Italy from June 10-16. Budget is tight.\"\n",
        "reply_1 = \"Noted. I'll find budget options for your Italy trip in June.\"\n",
        "\n",
        "print(\"Processing Turn 1...\")\n",
        "new_mems = extract_conversation_memories(turn_1, reply_1)\n",
        "travel_memory_store = update_memory_store(travel_memory_store, new_mems)\n",
        "\n",
        "print(\"\\nUpdated Store:\")\n",
        "print(json.dumps(travel_memory_store, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u6QaJNlHd6V"
      },
      "source": [
        "## 4.3 Memory-Augmented Responses (Personalization Loop) [10 points]\n",
        "Close the loop by reusing the stored facts:\n",
        "1. `summarize_relevant_memories` ‚Üí collapse the store into ‚â§5 bullet points that an LLM can skim quickly.\n",
        "2. `memory_aware_agent` ‚Üí prepend those bullets (when available) to the system instructions, remind the model to cite them explicitly, and fall back to the base prompt if the store is empty.\n",
        "3. After generating the follow-up reply, run a lightweight check (keyword search is fine) to confirm the answer referenced at least one stored fact.\n",
        "\n",
        "If the response ignores the memory summary, adjust the prompt and rerun.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "UgafaS7lHd6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ec8054-22f9-46ab-d3c9-6da94a531f36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User: We're going to Italy, June 10‚Äì16. Stay in Rome and Florence.\n",
            "Assistant: That sounds like a fantastic trip! Here‚Äôs a quick itinerary suggestion:\n",
            "\n",
            "Rome (June 10‚Äì13):\n",
            "- Day 1: Colosseum, Roman Forum, Trevi Fountain\n",
            "- Day 2: Vatican City (St. Peter‚Äôs Basilica, Sistine Chapel)\n",
            "- Day 3: Pantheon, Piazza Navona, Trastevere\n",
            "\n",
            "Florence (June 13‚Äì16):\n",
            "- Day 1: Duomo, Uffizi Gallery, Ponte Vecchio\n",
            "- Day 2: Accademia (David), Piazzale Michelangelo\n",
            "- Day 3: Day trip to Tuscany (wine tasting or Chianti region)\n",
            "\n",
            "Would you like recommendations for restaurants, hotels, or transportation?\n",
            "Memory topics: ['dates', 'destinations', 'lodging_preferences', 'activities']\n",
            "\n",
            "User: I'm allergic to shellfish, so find safe restaurants.\n",
            "Assistant: Here are shellfish-free restaurant recommendations in Rome and Florence:\n",
            "\n",
            "Rome:\n",
            "- Roscioli (Pasta, meat dishes)\n",
            "- Trattoria Da Enzo al 29 (Roman classics, ask for shellfish-free options)\n",
            "- La Carbonara (Pasta specialties, no seafood)\n",
            "\n",
            "Florence:\n",
            "- Trattoria Mario (Famous for steak, no seafood)\n",
            "- Osteria Santo Spirito (Tuscan cuisine, shellfish-free options)\n",
            "- All‚ÄôAntico Vinaio (Sandwiches, no seafood)\n",
            "\n",
            "Always confirm with the staff to ensure no cross-contamination. Safe travels!\n",
            "Memory topics: ['dates', 'destinations', 'lodging_preferences', 'activities', 'dietary', 'backup_plans']\n",
            "\n",
            "User: For the Rome leg, can you suggest a dinner spot?\n",
            "Assistant: For a memorable dinner in Rome, I recommend Roscioli‚Äîa renowned spot for pasta and meat dishes, with no shellfish on the menu. Their handmade pasta and wine selection are excellent. Alternatively, La Carbonara offers classic Roman pasta in a cozy setting. Both are highly rated and safe for your allergy.\n",
            "Memory topics: ['dates', 'destinations', 'lodging_preferences', 'activities', 'dietary', 'backup_plans']\n",
            "\n",
            "Final Memory State:\n",
            "- dates: June 10‚Äì16\n",
            "- destinations: Rome, Florence; Rome and Florence\n",
            "- lodging_preferences: Stay in Rome and Florence; No specific lodging preferences mentioned.\n",
            "- activities: Colosseum, Roman Forum, Trevi Fountain, Vatican City (St. Peter‚Äôs Basilica, Sistine Chapel), Pantheon, Piazza Navona, Trastevere, Duomo, Uffizi Gallery, Ponte Vecchio, Accademia (David), Piazzale Michelangelo, Day trip to Tuscany (wine tasting or Chianti region); Dining at shellfish-free restaurants.; dinner at Roscioli or La Carbonara\n",
            "- dietary: Allergic to shellfish, requires shellfish-free restaurants in Rome and Florence.; no shellfish on the menu\n"
          ]
        }
      ],
      "source": [
        "def summarize_memories(store):\n",
        "    \"\"\"Convert the memory dictionary into a text summary (max 5 bullets).\"\"\"\n",
        "    if not store:\n",
        "        return \"\"\n",
        "    bullets = [f\"- {topic}: {'; '.join(details)}\" for topic, details in store.items()]\n",
        "    return \"\\n\".join(bullets[:5])\n",
        "def run_memory_aware_conversation(base_system_prompt, user_turns):\n",
        "    \"\"\"Run a full conversation where the agent learns and remembers.\"\"\"\n",
        "    messages = [{\"role\": \"system\", \"content\": base_system_prompt}]\n",
        "    memory_store = {}\n",
        "\n",
        "    for user_msg in user_turns:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "\n",
        "        # Inject memory into system prompt if available\n",
        "        memory_summary = summarize_memories(memory_store)\n",
        "        if memory_summary:\n",
        "            system_with_memory = f\"{base_system_prompt}\\n\\nRemembered facts (cite these explicitly when relevant):\\n{memory_summary}\"\n",
        "            messages[0] = {\"role\": \"system\", \"content\": system_with_memory}\n",
        "\n",
        "        prompt = format_transcript_for_prompt(messages)\n",
        "        assistant_msg = basic_agent(prompt)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "\n",
        "        # Extract and store new memories\n",
        "        new_memories = extract_conversation_memories(user_msg, assistant_msg)\n",
        "        memory_store = update_memory_store(memory_store, new_memories)\n",
        "\n",
        "        print(f\"\\nUser: {user_msg}\")\n",
        "        print(f\"Assistant: {assistant_msg}\")\n",
        "        print(f\"Memory topics: {list(memory_store.keys())}\")\n",
        "\n",
        "        # Lightweight check: verify response references stored facts (keyword search)\n",
        "        if memory_store:\n",
        "            keywords = [detail.split()[0].lower() for details in memory_store.values() for detail in details if detail]\n",
        "            referenced = any(kw in assistant_msg.lower() for kw in keywords[:5])\n",
        "            if not referenced and len(messages) > 4:\n",
        "                print(\"(Note: Response may not have referenced stored memories)\")\n",
        "\n",
        "    return memory_store\n",
        "\n",
        "# --- Run the Scenario ---\n",
        "travel_turns = [\n",
        "  \"We're going to Italy, June 10‚Äì16. Stay in Rome and Florence.\",\n",
        "  \"I'm allergic to shellfish, so find safe restaurants.\",\n",
        "  \"For the Rome leg, can you suggest a dinner spot?\"\n",
        "]\n",
        "\n",
        "system_prompt = \"You are a helpful travel assistant.\"\n",
        "final_store = run_memory_aware_conversation(system_prompt, travel_turns)\n",
        "\n",
        "print(\"\\nFinal Memory State:\")\n",
        "print(summarize_memories(final_store))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}