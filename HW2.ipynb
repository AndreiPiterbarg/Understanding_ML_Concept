{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreiPiterbarg/Understanding_ML_Concept/blob/main/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NtBU7ePNoKns",
        "outputId": "cce16dd9-1e2d-4a9b-a9b3-a2258229fdaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 362, in run\n",
            "    resolver = self.make_resolver(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 177, in make_resolver\n",
            "    return pip._internal.resolution.resolvelib.resolver.Resolver(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 58, in __init__\n",
            "    self.factory = Factory(\n",
            "                   ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 129, in __init__\n",
            "    for dist in env.iter_installed_distributions(local_only=False)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/base.py\", line 664, in <genexpr>\n",
            "    return (d for d in it if d.canonical_name not in skip)\n",
            "                       ^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/base.py\", line 612, in iter_all_distributions\n",
            "    for dist in self._iter_distributions():\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 176, in _iter_distributions\n",
            "    yield from finder.find(location)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 79, in find\n",
            "    for dist, info_location in self._find_impl(location):\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 64, in _find_impl\n",
            "    raw_name = get_dist_name(dist)\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_compat.py\", line 52, in get_dist_name\n",
            "    name = cast(Any, dist).name\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/metadata/__init__.py\", line 457, in name\n",
            "    return self.metadata['Name']\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/metadata/__init__.py\", line 452, in metadata\n",
            "    return _adapters.Message(email.message_from_string(text))\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/email/__init__.py\", line 37, in message_from_string\n",
            "    return Parser(*args, **kws).parsestr(s)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/email/parser.py\", line 64, in parsestr\n",
            "    return self.parse(StringIO(text), headersonly=headersonly)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/email/parser.py\", line 53, in parse\n",
            "    feedparser.feed(data)\n",
            "  File \"/usr/lib/python3.12/email/feedparser.py\", line 174, in feed\n",
            "    self._call_parse()\n",
            "  File \"/usr/lib/python3.12/email/feedparser.py\", line 178, in _call_parse\n",
            "    self._parse()\n",
            "  File \"/usr/lib/python3.12/email/feedparser.py\", line 254, in _parsegen\n",
            "    if self._cur.get_content_type() == 'message/delivery-status':\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/email/message.py\", line 614, in get_content_type\n",
            "    value = self.get('content-type', missing)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/email/message.py\", line 506, in get\n",
            "    if k.lower() == name:\n",
            "       ^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 216, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1527, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1684, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1700, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1762, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1028, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.12/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1280, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1160, in emit\n",
            "    msg = self.format(record)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 999, in format\n",
            "    return fmt.format(record)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 711, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 661, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 124, in print_exception\n",
            "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 733, in __init__\n",
            "    self.stack = StackSummary._extract_from_extended_frame_gen(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 438, in _extract_from_extended_frame_gen\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 323, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/linecache.py\", line 30, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/linecache.py\", line 46, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/linecache.py\", line 141, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/tokenize.py\", line 459, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/tokenize.py\", line 428, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "            ^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/tokenize.py\", line 386, in read_or_stop\n",
            "    return readline()\n",
            "           ^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "%pip install mistralai --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqN8LAJ8tGq4"
      },
      "source": [
        "# Mastering Prompt Engineering\n",
        "In this notebook, you will practice systematically improving prompts to get better results from a Large Language Model.\n",
        "\n",
        "The goal of this assignment is to move from a vague, \"bad\" prompt to a precise, well-structured prompt that elicits a much more accurate and useful response from the model. You will also experiment with techniques that encourage the model to \"reason\" its way to a better answer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK2Y0VrPgGkG"
      },
      "source": [
        "## Part 1: Setting up a Mistral agent [5 points]\n",
        "To use the Mistral API, you need an API key.\n",
        "\n",
        "1. Create or sign in to your account at [console.mistral.ai](https://console.mistral.ai/).\n",
        "2. Open the **API Keys** tab and click **Create API key**. Give it a memorable name and copy the value.\n",
        "3. In this Colab notebook, click on the \"üîë\" (key) icon in the left sidebar.\n",
        "4. Click \"Add new secret\". Name the secret `MISTRAL_API_KEY` and paste your key into the \"Value\" field.\n",
        "5. Make sure the \"Notebook access\" toggle is turned on.\n",
        "\n",
        "By using Colab secrets, you keep your API key secure and avoid pasting it directly into your code. The code cell below will access this secret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pci-vhgJf_ee"
      },
      "outputs": [],
      "source": [
        "from mistralai import Mistral\n",
        "from google.colab import userdata\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Configure the API key\n",
        "try:\n",
        "    api_key = userdata.get('MISTRAL_API_KEY')\n",
        "    client = Mistral(api_key=api_key)\n",
        "    print(\"API Key configured successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring API key: {e}\")\n",
        "    print(\"Please make sure you have set the 'MISTRAL_API_KEY' secret in your Colab environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t28G6JY5sK_u"
      },
      "source": [
        "## 1.1 Agent Setup [5 points]\n",
        "Create a basic agent that sends a prompt to the Mistral model and returns a cleaned response. You should do the following:\n",
        "1. Create a generation config to set the temperature.\n",
        "2. Process the response to remove markdown formatting - any \"*\" or \"#\" characters\n",
        "3. Return the cleaned response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMXQmQ_ag4cL"
      },
      "outputs": [],
      "source": [
        "# Initialize the Mistral client (do not change the model name)\n",
        "MODEL_NAME = \"mistral-small-latest\"\n",
        "\n",
        "def basic_agent(prompt: str, temperature=0.0) -> str:\n",
        "  \"\"\"Send a prompt to Mistral and return a markdown-free string response.\"\"\"\n",
        "  # TODO\n",
        "  return \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYD_qbJhsjDp"
      },
      "source": [
        "## Part 2: Iterating and Improving a \"Bad\" Prompt [15 points]\n",
        "\n",
        "An LLM's output is only as good as the prompt it receives. Vague prompts lead to vague or incorrect answers. In this module, you will start with a deliberately \"bad\" prompt and improve it.\n",
        "\n",
        "**Task: Extract structured information from a block of text**\n",
        "\n",
        "Our goal is to process a chaotic meeting transcript from a supervillain's weekly sync. Evil plans have logistics too, and they're often messy. Your task is to extract key details and action items into a nested JSON object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzcSXkVMy65G"
      },
      "outputs": [],
      "source": [
        "supervillain_transcript = \"\"\"\n",
        "Project Doom-Spire - Weekly Status Update - Monday, February 9, 2025\n",
        "\n",
        "Attendees: Dr. Dire (Evil Overlord), Brenda (Lair HR & Logistics), Gary (Lead Henchman), Chad (Intern)\n",
        "\n",
        "Dr. Dire: People! The Doom-Spire is 80% complete, but I stood on the Parapet of Pain this morning and... it felt bland! It needs more... menace! Chad, you're the intern. Your youthful apathy is in tune with modern aesthetics. I want you to add more skulls to the parapet. Many more. Make it happen.\n",
        "\n",
        "Brenda: Um, Doctor? Speaking of making things happen, Unreliable Aquatics Inc. just called. The laser-equipped sharks for the moat are delayed. Again. They said \"next month, maybe.\" This is blocking the entire moat-filling initiative.\n",
        "\n",
        "Dr. Dire: Unacceptable! Brenda, find me a new shark supplier, ASAP. I don't care what it takes. Double our budget if you have to. I want sharks with lasers, and I want them yesterday!\n",
        "\n",
        "Gary: While we're on blockers, the primary laser core is overheating. It keeps melting the containment unit, which is, you know, suboptimal. We need a new cryo-cooler installed or the whole spire might just... pop.\n",
        "\n",
        "Dr. Dire: Pop? Gary, \"pop\" is not a word I want associated with my multi-billion dollar evil lair. Fine. Get the cooler. When can it be done?\n",
        "\n",
        "Gary: My team can have the new unit fully installed by this Friday.\n",
        "\n",
        "Dr. Dire: Good. See to it that it's done. I want no more talk of 'popping'. Okay team, good sync. Let's get back to menacing the world!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP1z2DCBy1Or"
      },
      "outputs": [],
      "source": [
        "# Version 1: The \"Bad\" Prompt\n",
        "bad_prompt = f\"\"\"\n",
        "What happened in this meeting?\n",
        "\n",
        "Transcript:\n",
        "{supervillain_transcript}\n",
        "\"\"\"\n",
        "\n",
        "output = basic_agent(bad_prompt)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv2wXs8yzBRE"
      },
      "source": [
        "### 2.1 Adding Specificity and Structure [5 points]\n",
        "Version 1 summarizes the meeting but is impossible to parse. Rewrite the prompt in the next cell so the model returns the information in a nested JSON structure. We need the project name, a list of attendees with their titles, and a list of action items, where each action item is its own object.\n",
        "\n",
        "**Prompting Technique: Specificity and Schema Definition. Clearly define the entire output schema, including nested objects and arrays.**\n",
        "\n",
        "Your Task: Modify the prompt below to ask for a JSON object with the keys `project_name`, `attendees` (a list of objects with name and title), and `action_items` (a list of objects, where each object has `task_description`, `assigned_to`, and `due_date`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0bc7598"
      },
      "source": [
        "Hint: For reliable JSON, see Mistral‚Äôs Structured Outputs docs ([overview](https://docs.mistral.ai/capabilities/structured_output), [custom schemas](https://docs.mistral.ai/capabilities/structured_output/custom)) and JSON mode ([json mode](https://docs.mistral.ai/capabilities/structured_output/json_mode))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o9BZR9ty7lr"
      },
      "outputs": [],
      "source": [
        "# Version 2: Specifying the Nested Schema\n",
        "schema_prompt = f\"\"\"\n",
        "TODO: Write your prompt here.\n",
        "\n",
        "Transcript:\n",
        "{supervillain_transcript}\n",
        "\"\"\"\n",
        "\n",
        "output = basic_agent(schema_prompt)\n",
        "\n",
        "# Clean the output for JSON parsing\n",
        "try:\n",
        "    cleaned_output = re.search(r'```json\\n(.*)\\n```', output, re.DOTALL).group(1)\n",
        "    parsed_json = json.loads(cleaned_output)\n",
        "    print(\"\\n‚úÖ Final JSON Output:\\n\")\n",
        "    print(json.dumps(parsed_json, indent=2))\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Output is NOT valid nested JSON. Raw output:\\n{output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJBmINdNzNSX"
      },
      "source": [
        "### 2.2 Handling Ambiguity and Inference [5 points]\n",
        "The schema is correct, but the model still struggles with ambiguity. Update the prompt in the next cell so it states the following rules explicitly:\n",
        "1. Treat Monday, February 9, 2025 as the reference date for every due date calculation.\n",
        "2. Map \"ASAP\" to the next business day.\n",
        "3. Use `null` when no deadline appears in the transcript.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PyRGBPJzSCu"
      },
      "outputs": [],
      "source": [
        "# Version 3: Handling Ambiguity\n",
        "ambiguity_prompt = f\"\"\"\n",
        "TODO: Write your prompt here. Your instructions must enforce that the meeting happened on Monday, February 9, 2025, \"ASAP\" deadlines resolve to Tuesday, February 10, 2025, and \"by Friday\" deadlines resolve to Friday, February 14, 2025. Output dates in YYYY-MM-DD format and set `due_date` to null when no deadline is provided.\n",
        "\n",
        "Transcript:\n",
        "{supervillain_transcript}\n",
        "\"\"\"\n",
        "\n",
        "output = basic_agent(ambiguity_prompt)\n",
        "\n",
        "# Clean the output for JSON parsing\n",
        "try:\n",
        "    # The model sometimes wraps the JSON in markdown ` ```json ... ``` `\n",
        "    cleaned_output = re.search(r'```json\\n(.*)\\n```', output, re.DOTALL).group(1)\n",
        "    parsed_json = json.loads(cleaned_output)\n",
        "    print(\"\\n‚úÖ Final JSON Output:\\n\")\n",
        "    print(json.dumps(parsed_json, indent=2))\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Output is NOT valid nested JSON. Raw output:\\n{output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-chT70r5QdB"
      },
      "source": [
        "### 2.3 Automatic Prompt Engineering (Meta-Prompting) [5 points]\n",
        "The final step is to automate the improvements you just made. You will:\n",
        "- write a meta-prompt that teaches `prompt_agent` how to craft the final schema-aware prompt\n",
        "- require the meta-prompt to mention the persona, schema, reference date, ambiguity rules, and JSON-only constraint\n",
        "- append the transcript to the generated prompt before calling `basic_agent`\n",
        "\n",
        "Effectively, you are building a helper agent. Make the instructions so unambiguous that a second model can follow them without you editing the prompt by hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWILjssxRU5b"
      },
      "outputs": [],
      "source": [
        "def prompt_agent(meta_prompt: str, transcript: str) -> str:\n",
        "  \"\"\"TODO: Generate a refined prompt string and return it with the transcript appended.\"\"\"\n",
        "  # TODO\n",
        "  return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDPXKZ2GR9Xk"
      },
      "source": [
        "**Your Task:** Fill in `meta_prompt` so that `prompt_agent(meta_prompt, supervillain_transcript)` produces a final prompt containing all of the following:\n",
        "1. A persona assignment (e.g., expert project manager bot).\n",
        "2. A clear restatement of the goal: extract structured JSON.\n",
        "3. The exact schema: `project_name`, `attendees` with `name` + `title`, and `action_items` with `task_description`, `assigned_to`, `due_date`.\n",
        "4. The reference date (Monday, February 9, 2025).\n",
        "5. The ambiguity rules for \"ASAP\" and \"by Friday\" and `null` for missing dates.\n",
        "6. An explicit instruction to output JSON only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcpuzi1BSTeB"
      },
      "outputs": [],
      "source": [
        "meta_prompt = \"\"\"\n",
        "  TODO: Write your meta-prompt here\n",
        "\"\"\"\n",
        "\n",
        "# --- Let's run the two-step process ---\n",
        "\n",
        "# Step 1: Use the meta-prompt to have the prompt_agent generate a new prompt for us.\n",
        "print(\"--- Generating a new prompt using the prompt_agent... ---\")\n",
        "generated_prompt = prompt_agent(meta_prompt, supervillain_transcript)\n",
        "print(generated_prompt)\n",
        "print(\"----------------------------------------------------\")\n",
        "\n",
        "\n",
        "# Step 2: Use the newly generated prompt to process the transcript with our original agent.\n",
        "print(\"\\n\\n--- Using the generated prompt to extract data... ---\")\n",
        "# We need to append the transcript to the prompt that was just generated\n",
        "final_output = basic_agent(generated_prompt)\n",
        "\n",
        "# Clean the output for JSON parsing\n",
        "try:\n",
        "    cleaned_output = re.search(r'```json\\n(.*)\\n```', final_output, re.DOTALL).group(1)\n",
        "    parsed_json = json.loads(cleaned_output)\n",
        "    print(\"\\n‚úÖ Final JSON Output:\\n\")\n",
        "    print(json.dumps(parsed_json, indent=2))\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Output is NOT valid nested JSON. Raw output:\\n{final_output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPEykj9848Iz"
      },
      "source": [
        "# Part 3: Eliciting Reasoning [20 points]\n",
        "You will practice two reasoning-friendly prompt patterns: Chain-of-Thought for constraint solving and Reflection for self-critique. Each sub-part has a single correct answer, so your prompts must spell out how the model should reason before giving the final response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J4I_4sZHd6T"
      },
      "source": [
        "## 3.1: Chain-of-Thought (CoT) for Complex Problems [10 points]\n",
        "\n",
        "For complex problems, like logic puzzles, simply asking for the answer can be unreliable. The Chain-of-Thought (CoT) technique guides the model to \"think step-by-step,\" breaking down the problem and showing its work. This reasoning process dramatically increases the likelihood of arriving at the correct answer.\n",
        "\n",
        "The Task: Solve a moderately difficult logic puzzle with a single correct solution.\n",
        "\n",
        "*Four wizards are discussing their magical pets. The wizards are Arthur, Beatrice, Cassandra, and Desmond. The pets are an Owl, a Griffin, a Phoenix, and a Dragon. Each wizard owns exactly one pet*\n",
        "* *Constraint 1: Beatrice owns the Dragon.*\n",
        "* *Constraint 2: The owner of the Griffin is not Cassandra or Desmond.*\n",
        "* *Constraint 3: Arthur does not own the Phoenix.*\n",
        "* *Constraint 4: Cassandra's pet is not the Owl.*\n",
        "\n",
        "**Your Task: Write a prompt that uses the Chain-of-Thought technique to solve this puzzle. The prompt should instruct the model to first lay out the facts, then use a process of elimination, and finally state the answer in a JSON format.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP-GOqRD5HcM"
      },
      "outputs": [],
      "source": [
        "logic_puzzle = \"\"\"\n",
        "Four wizards are discussing their magical pets. The wizards are Arthur, Beatrice, Cassandra, and Desmond. The pets are an Owl, a Griffin, a Phoenix, and a Dragon. Each wizard owns exactly one pet.\n",
        "- Constraint 1: Beatrice owns the Dragon.\n",
        "- Constraint 2: The owner of the Griffin is not Cassandra or Desmond.\n",
        "- Constraint 3: Arthur does not own the Phoenix.\n",
        "- Constraint 4: Cassandra's pet is not the Owl.\n",
        "\"\"\"\n",
        "\n",
        "cot_puzzle_prompt = f\"\"\"\n",
        "  TODO: Write your chain-of-thought prompt here.\n",
        "\n",
        "Puzzle:{logic_puzzle}\n",
        "\"\"\"\n",
        "\n",
        "output = basic_agent(cot_puzzle_prompt)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFjYVuafgZC8"
      },
      "source": [
        "## 3.2 Reflection and Refinement (Self-Correction) [10 points]\n",
        "Write a single prompt that makes the model critique its own startup idea in three clear passes:\n",
        "1. *Step 1 ‚Äì Ideation:* act as an optimistic founder, name the app, and describe the core function in one sentence.\n",
        "2. *Step 2 ‚Äì Skeptical VC:* switch personas and answer the classic diligence questions (problem urgency, current alternatives, market size, competitors & advantage, monetization).\n",
        "3. *Step 3 ‚Äì Improved Concept:* return to the founder persona, fix every weakness the VC raised, and explain why the new version is more defensible.\n",
        "\n",
        "**Your Task: Create a three prompts, one for each step of the process. Call the basic_agent three times to perform each step of this process**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLhm8vTPBSam"
      },
      "outputs": [],
      "source": [
        "step1_prompt = \"\"\"\n",
        "  TODO: Write the Step 1 ideation prompt described above.\n",
        "\"\"\"\n",
        "\n",
        "step1_output = basic_agent(step1_prompt)\n",
        "print(step1_output)\n",
        "\n",
        "step2_prompt = f\"\"\"\n",
        "  TODO: Write the Step 2 skeptical VC prompt using the critique checklist.\n",
        "\n",
        "  Concept to critique:\n",
        "  {step1_output}\n",
        "\"\"\"\n",
        "\n",
        "step2_output = basic_agent(step2_prompt)\n",
        "print(\"\\n\" + step2_output)\n",
        "\n",
        "step3_prompt = f\"\"\"\n",
        "  TODO: Write the Step 3 improved concept prompt that references both outputs.\n",
        "\n",
        "  Original concept:\n",
        "  {step1_output}\n",
        "\n",
        "  VC critique:\n",
        "  {step2_output}\n",
        "\"\"\"\n",
        "\n",
        "step3_output = basic_agent(step3_prompt)\n",
        "print(\"\\n\" + step3_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR3F5tiVBSam"
      },
      "source": [
        "# Part 4: Conversational Memory [30 points]\n",
        "You will build a three-layer travel concierge stack:\n",
        "1. Buffer the entire conversation so every reply remembers earlier preferences.\n",
        "2. Convert those turns into a structured memory store of durable trip facts.\n",
        "3. Feed the memory summaries back into the next response so the concierge stays consistent.\n",
        "\n",
        "Complete the steps in order‚Äîeach helper you write is reused in the following section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5pjLGS1BSam"
      },
      "source": [
        "## 4.1 Conversation Buffer [10 points]\n",
        "Implement two helpers that keep the running dialogue intact:\n",
        "- `format_transcript_for_prompt(messages)` ‚Üí return a newline-delimited string of labeled turns plus an instruction telling the assistant to continue.\n",
        "- `run_buffered_conversation(system_context, user_turns)` ‚Üí step through each user turn, rebuild the prompt with the full transcript, call `basic_agent`, store every reply, and return both the transcript and a dialogue log.\n",
        "\n",
        "Then, run the provided travel scenario and print both a single-shot response and the buffered dialogue so you can highlight why the buffered agent performs better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWdf7SviBSan"
      },
      "outputs": [],
      "source": [
        "def format_transcript_for_prompt(messages):\n",
        "    \"\"\"TODO: Return a speaker-labeled transcript string that becomes the next prompt.\"\"\"\n",
        "    # TODO\n",
        "\n",
        "\n",
        "def run_buffered_conversation(system_context, user_turns):\n",
        "    \"\"\"TODO: Return the updated transcript and dialogue history after iterating over every user turn.\"\"\"\n",
        "    # TODO\n",
        "\n",
        "\n",
        "travel_system_prompt = (\n",
        "  \"You are a detail-oriented travel concierge. Track traveler dates, transit rules, dietary needs, and must-see spots so itineraries stay consistent. Keep replies under 120 words.\"\n",
        ")\n",
        "travel_turns = [\n",
        "  \"Hi, we're planning a June 10‚Äì16 trip to Italy. We'd like three nights in Rome and three in Florence.\",\n",
        "  \"Please avoid redeye flights, schedule trains before 8pm, and remember I'm allergic to shellfish.\",\n",
        "  \"We love boutique hotels near museums and want one day trip to Tuscany with a local cooking class. Can you sketch the plan?\"\n",
        "]\n",
        "\n",
        "buffered_transcript, buffered_dialogue = run_buffered_conversation(\n",
        "  travel_system_prompt,\n",
        "  travel_turns\n",
        ")\n",
        "\n",
        "single_shot_prompt = (\n",
        "    f\"{travel_system_prompt}\\n\"\n",
        "    f\"User: {travel_turns[-1]}\\n\"\n",
        "    \"Assistant:\"\n",
        ")\n",
        "single_shot_response = basic_agent(single_shot_prompt)\n",
        "\n",
        "print(\"Single-shot response (no buffer):\\n\")\n",
        "print(single_shot_response)\n",
        "print(\"\\n---\\n\")\n",
        "print(\"Buffered dialogue:\")\n",
        "for idx, turn in enumerate(buffered_dialogue, start=1):\n",
        "    print(f\"Turn {idx} | User: {turn['user']}\")\n",
        "    print(f\"Turn {idx} | Assistant: {turn['assistant']}\\n\")\n",
        "print(\"Final buffered reply clearly references the earlier turns.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMnss5WDBSan"
      },
      "source": [
        "## 4.2 Structured Memory Store (Fact Capture) [10 points]\n",
        "Now that buffering works, capture persistent travel facts after every turn.\n",
        "- `extract_conversation_memories(user_message, assistant_message)` ‚Üí call the model to emit a JSON list of durable facts.\n",
        "- `update_memory_store(store, memories)` ‚Üí merge those facts by topic while deduplicating details.\n",
        "- `run_memory_capture_session(system_context, user_turns)` ‚Üí reuse the buffer helpers, call the extractor after each reply, print the evolving store, and return both the transcript and the final `memory_store`.\n",
        "\n",
        "Aim for a store that records dates, budgets, dietary needs, transit rules, and backup plans mentioned in the scenario.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKVGIpIiHd6V"
      },
      "outputs": [],
      "source": [
        "def extract_conversation_memories(user_message, assistant_message):\n",
        "    \"\"\"Use an LLM to extract key facts into a JSON list of objects: [{'topic': ..., 'detail': ...}].\"\"\"\n",
        "    # TODO\n",
        "\n",
        "def update_memory_store(store, memories):\n",
        "    \"\"\"Merge new memories into the store, avoiding duplicates.\"\"\"\n",
        "    # TODO\n",
        "\n",
        "# --- Test Logic ---\n",
        "travel_transcript = []\n",
        "travel_memory_store = {}\n",
        "\n",
        "turn_1 = \"I'm planning a trip to Italy from June 10-16. Budget is tight.\"\n",
        "reply_1 = \"Noted. I'll find budget options for your Italy trip in June.\"\n",
        "\n",
        "print(\"Processing Turn 1...\")\n",
        "new_mems = extract_conversation_memories(turn_1, reply_1)\n",
        "travel_memory_store = update_memory_store(travel_memory_store, new_mems)\n",
        "\n",
        "print(\"\\nUpdated Store:\")\n",
        "print(json.dumps(travel_memory_store, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u6QaJNlHd6V"
      },
      "source": [
        "## 4.3 Memory-Augmented Responses (Personalization Loop) [10 points]\n",
        "Close the loop by reusing the stored facts:\n",
        "1. `summarize_relevant_memories` ‚Üí collapse the store into ‚â§5 bullet points that an LLM can skim quickly.\n",
        "2. `memory_aware_agent` ‚Üí prepend those bullets (when available) to the system instructions, remind the model to cite them explicitly, and fall back to the base prompt if the store is empty.\n",
        "3. After generating the follow-up reply, run a lightweight check (keyword search is fine) to confirm the answer referenced at least one stored fact.\n",
        "\n",
        "If the response ignores the memory summary, adjust the prompt and rerun.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgafaS7lHd6V"
      },
      "outputs": [],
      "source": [
        "def summarize_memories(store):\n",
        "    \"\"\"Convert the memory dictionary into a text summary for the prompt.\"\"\"\n",
        "    # TODO\n",
        "\n",
        "def run_memory_aware_conversation(base_system_prompt, user_turns):\n",
        "    \"\"\"Run a full conversation where the agent learns and remembers.\"\"\"\n",
        "    # TODO\n",
        "\n",
        "# --- Run the Scenario ---\n",
        "travel_turns = [\n",
        "  \"We're going to Italy, June 10‚Äì16. Stay in Rome and Florence.\",\n",
        "  \"I'm allergic to shellfish, so find safe restaurants.\",\n",
        "  \"For the Rome leg, can you suggest a dinner spot?\"\n",
        "]\n",
        "\n",
        "system_prompt = \"You are a helpful travel assistant.\"\n",
        "final_store = run_memory_aware_conversation(system_prompt, travel_turns)\n",
        "\n",
        "print(\"\\nFinal Memory State:\")\n",
        "print(summarize_memories(final_store))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}